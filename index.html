<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Oswin So </title> <meta name="author" content="Oswin So"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="robotics, machine-learning, hamilton-jacobi, control-barrier-function"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%94%A5%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://oswinso.github.io/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item"> <a class="nav-link" href="https://oswinso.github.io/assets/pdf/oswin_resume_2026.02.09.pdf" target="\_blank">CV</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Oswin</span> So </h1> <p class="desc">4th year PhD @ <a href="https://aeroastro.mit.edu/realm" rel="external nofollow noopener" target="_blank">REALM</a>, MIT AeroAstro </p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/profile-480.webp 480w,/assets/img/profile-800.webp 800w,/assets/img/profile-1400.webp 1400w," type="image/webp" sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/profile.jpeg?v=0103b4827dc8b79bbbb2880ce485531e" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="profile.jpeg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="clearfix"> <p>I am interested in developing structure-exploiting algorithms for the learning and control of dynamical systems. My research has mainly focused on improving <strong>safety in reinforcement learning using Hamilton-Jacobi reachability analysis and Control Barrier Functions</strong>, with applications to multi-agent systems and fixed wing aircraft. I have also dabbled in topics such as <strong>scientific reasoning in (d)LLMs</strong>.</p> <p>I’m currently at <a href="https://aeroastro.mit.edu/realm" rel="external nofollow noopener" target="_blank">REALM</a> at MIT, advised by <a href="https://chuchu.mit.edu/" rel="external nofollow noopener" target="_blank">Chuchu Fan</a>. Previously, I did my undergrad at Georgia Tech, where I was very fortunate to do undergraduate research with <a href="https://scholar.google.com/citations?user=dG9MV7oAAAAJ&amp;hl=en" target="_blank" rel="external nofollow noopener">Evangelos Theodorou</a> and <a href="https://mtao8.math.gatech.edu" target="_blank" rel="external nofollow noopener">Molei Tao</a>.</p> <p>The past summer, I interned at META FAIR working on fine-tuning discrete generative models characterized by Continuous-Time Markov Chains (e.g., diffusion-based LLMs), where I was mentored by <a href="https://ghliu.github.io" target="_blank" rel="external nofollow noopener">Guan-Horng Liu</a> and worked with <a href="https://rtqichen.github.io" rel="external nofollow noopener" target="_blank">Ricky T. Q. Chen</a>. I’ve previously interned at Toyota Research Institute, where I worked on game theoretic planning. I also worked at Aurora as a Behavior Planning Intern during the summer of 2021 under <a href="https://scholar.google.com/citations?user=daYjNkAAAAAJ&amp;hl=en" target="_blank" rel="external nofollow noopener">Paul Vernaza</a> and <a href="https://scholar.google.com/citations?user=Nt4rO1EAAAAJ&amp;hl=en" target="_blank" rel="external nofollow noopener">Arun Venkatraman</a>, working on cost function learning via on-policy negative examples for autonomous driving.</p> <p>See my <b> <a href="https://oswinso.github.io/assets/pdf/oswin_resume_2026.02.09.pdf" target="_blank">full CV</a> </b> here (updated in February 2026).</p> <p><strong>Contact: </strong> oswinso [at] mit [dot] edu <br> <strong>Follow: </strong> <a href="https://scholar.google.com/citations?user=AwlxGQgAAAAJ" target="_blank" title="Google Scholar" rel="external nofollow noopener"><i class="ai ai-google-scholar"></i> Google Scholar</a> <strong> | </strong> <a href="https://www.linkedin.com/in/oswinso" target="_blank" title="LinkedIn" rel="external nofollow noopener"><i class="fab fa-linkedin"></i> LinkedIn</a> <strong> | </strong> <a href="https://github.com/oswinso" target="_blank" title="GitHub" rel="external nofollow noopener"><i class="fab fa-github"></i> oswinso</a> <strong> | </strong> <a href="https://twitter.com/oswinso" target="_blank" title="GitHub" rel="external nofollow noopener"><i class="fab fa-twitter"></i> @oswinso</a> <br><br></p> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="explanation"> (* Equal contribution. See <a href="https://scholar.google.com/citations?user=AwlxGQgAAAAJ" target="_blank" title="Google Scholar" rel="external nofollow noopener">Google Scholar</a> for the full list.) </div> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">In Submission</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/vdppo-480.webp 480w,/assets/img/publication_preview/vdppo-800.webp 800w,/assets/img/publication_preview/vdppo-1400.webp 1400w," type="image/webp" sizes="1000px"> <img src="/assets/img/publication_preview/vdppo.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="vdppo.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="sharpless2026bellman" class="col-sm-8"> <div class="title">Bellman Value Decomposition for Task Logic in Safe Optimal Control</div> <div class="author"> William Sharpless<sup>*</sup>, <em>Oswin So<sup>*</sup></em>, Dylan Hirsch, Sylvia Herbert, and Chuchu Fan </div> <div class="periodical"> <em>In Submission</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://oswinso.xyz/assets/publications/vdppo.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Real-world tasks involve nuanced combinations of goal and safety specifications, which often directly compete. In high dimensions, the challenge is exacerbated: formal automata become cumbersome, and the combination of sparse rewards tends to require laborious tuning. In this work, we consider the structure of the Bellman Value as a means to naturally organize the problem for improved automatic performance without introducing additional abstractions. Namely, we prove the Bellman Value for a complex task defined in temporal logic can be decomposed into a graph of Bellman Values, where the graph is connected by a set of well-studied Bellman equations (BEs): the Reach-Avoid BE, the Avoid BE, and a novel type, the Reach-Avoid-Loop BE. From this perspective, we design a specialized PPO variant, Value-Decomposition PPO (VDPPO) that uses a single learned representation by embedding the decomposed Value graph. We conduct a variety of simulated and real multi-objective experiments, including delivery and herding, to test our method on diverse high-dimensional systems involving heterogeneous teams and complex agents. Ultimately, we find this approach greatly improves performance over existing baselines, balancing safety and liveness automatically. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR 2026</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/dam-480.webp 480w,/assets/img/publication_preview/dam-800.webp 800w,/assets/img/publication_preview/dam-1400.webp 1400w," type="image/webp" sizes="1000px"> <img src="/assets/img/publication_preview/dam.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="dam.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="so2026discrete" class="col-sm-8"> <div class="title">Discrete Adjoint Matching</div> <div class="author"> <em>Oswin So</em>, Brian Karrer, Chuchu Fan, Ricky T. Q. Chen, and Guan-Horng Liu </div> <div class="periodical"> <em>In The Fourteenth International Conference on Learning Representations (ICLR)</em>, 2026 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://oswinso.xyz/assets/publications/dam.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://oswinso.xyz/dam" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Computation methods for solving entropy-regularized reward optimization—a class of problems widely used for fine-tuning generative models—have advanced rapidly. Among those, Adjoint Matching (AM, Domingo-Enrich et al., 2025) has proven highly effective in continuous state spaces with differentiable rewards. Transferring these practical successes to discrete generative modeling, however, remains particularly challenging and largely unexplored, mainly due to the drastic shift in generative model classes to discrete state spaces, which are nowhere differentiable. In this work, we propose Discrete Adjoint Matching (DAM)—a discrete variant of AM for fine-tuning discrete generative models characterized by Continuous-Time Markov Chains, such as diffusion-based large language models. The core of DAM is the introduction of discrete adjoint—an estimator of the optimal solution to the original problem but formulated on discrete domains—from which standard matching frameworks can be applied. This is derived via a purely statistical standpoint, in contrast to the control-theoretic viewpoint in AM, thereby opening up new algorithmic opportunities for general adjoint-based estimators. We showcase DAM’s effectiveness on synthetic and mathematical reasoning tasks. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR 2026</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/fge-480.webp 480w,/assets/img/publication_preview/fge-800.webp 800w,/assets/img/publication_preview/fge-1400.webp 1400w," type="image/webp" sizes="1000px"> <img src="/assets/img/publication_preview/fge.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="fge.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="so2026solving" class="col-sm-8"> <div class="title">Solving Parameter-Robust Avoid Problems with Unknown Feasibility using Reinforcement Learning</div> <div class="author"> <em>Oswin So<sup>*</sup></em>, Eric Yang Yu<sup>*</sup>, Songyuan Zhang, Matthew Cleaveland, Mitchell Black, and Chuchu Fan </div> <div class="periodical"> <em>In The Fourteenth International Conference on Learning Representations (ICLR)</em>, 2026 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://oswinso.xyz/assets/publications/fge.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://oswinso.xyz/fge" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Recent advances in deep reinforcement learning (RL) have achieved strong results on high-dimensional control tasks, but applying RL to reachability problems raises a fundamental mismatch: reachability seeks to maximize the set of states from which a system remains safe indefinitely, while RL optimizes expected returns over a user-specified distribution. This mismatch can result in policies that perform poorly on low-probability states that are still within the safe set. A natural alternative is to frame the problem as a robust optimization over a set of initial conditions that specify the initial state, dynamics and safe set, but whether this problem has a solution depends on the feasibility of the specified set, which is unknown a priori. We propose Feasibility-Guided Exploration (FGE), a method that simultaneously identifies a subset of feasible initial conditions under which a safe policy exists, and learns a policy to solve the reachability problem over this set of initial conditions. Empirical results demonstrate that FGE learns policies with over 50% more coverage than the best existing method for challenging initial conditions across tasks in the MuJoCo simulator and the Kinetix simulator with pixel observations. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR 2026</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/reform-480.webp 480w,/assets/img/publication_preview/reform-800.webp 800w,/assets/img/publication_preview/reform-1400.webp 1400w," type="image/webp" sizes="1000px"> <img src="/assets/img/publication_preview/reform.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="reform.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="zhang2026reflected" class="col-sm-8"> <div class="title">ReFORM: Reflected Flows for On-support Offline RL via Noise Manipulation</div> <div class="author"> Songyuan Zhang, <em>Oswin So</em>, H. M. Sabbir Ahmad, Eric Yang Yu, Matthew Cleaveland, Mitchell Black, and Chuchu Fan </div> <div class="periodical"> <em>In The Fourteenth International Conference on Learning Representations (ICLR)</em>, 2026 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2602.05051" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/MIT-REALM/reform" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://mit-realm.github.io/reform/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Offline reinforcement learning (RL) aims to learn the optimal policy from a fixed behavior policy dataset without additional environment interaction. One com- mon challenge that arises in this setting is the out-of-distribution (OOD) error, which occurs when the policy leaves the training distribution. Prior methods pe- nalize a statistical distance term to keep the policy close to the behavior policy, but this constrains policy improvement and may not completely prevent OOD actions. Another challenge is that the optimal policy distribution can be multimodal and difficult to represent. Recent works apply diffusion or flow policies to address this problem, but it is unclear how to avoid OOD errors while retaining policy expres- siveness. We propose ReFORM, an offline RL method based on flow policies that enforces the less restrictive support constraint by construction. ReFORM learns a BC flow policy with a bounded source distribution to capture the support of the action distribution, then optimizes a reflected flow that generates bounded noise for the BC flow while keeping the support, to maximize the performance. Across 40 challenging tasks from the OGBench benchmark with datasets of varying qual- ity and using a constant set of hyperparameters for all tasks, ReFORM dominates all baselines with hand-tuned hyperparameters on the performance profile curves.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">RSS 2025</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/defmarl-480.webp 480w,/assets/img/publication_preview/defmarl-800.webp 800w,/assets/img/publication_preview/defmarl-1400.webp 1400w," type="image/webp" sizes="1000px"> <img src="/assets/img/publication_preview/defmarl.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="defmarl.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="zhang2025solving" class="col-sm-8"> <div class="title">Solving Multi-Agent Safe Optimal Control with Distributed Epigraph Form MARL</div> <div class="author"> Songyuan Zhang<sup>*</sup>, <em>Oswin So<sup>*</sup></em>, Mitchell Black, Zachary Serlin, and Chuchu Fan </div> <div class="periodical"> <em>In Robotics: Science and Systems (RSS)</em>, 2025 </div> <div class="periodical"> </div> <span class="awardNote">[Outstanding Student Paper Award, 0.6%]</span> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2504.15425" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/MIT-REALM/def-marl/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://mit-realm.github.io/def-marl/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Outstanding Student Paper Award, 0.6%</p> </div> <div class="abstract hidden"> <p>Tasks for multi-robot systems often require the robots to collaborate and complete a team goal while maintaining safety. This problem is usually formalized as a Constrained Markov decision process (CMDP), which targets minimizing a global cost and bringing the mean of constraint violation below a user-defined threshold. Inspired by real-world robotic applications, we define safety as zero constraint violation. While many safe multi-agent reinforcement learning (MARL) algorithms have been proposed to solve CMDPs, these algorithms suffer from unstable training in this setting. To tackle this, we use the epigraph form for constrained optimization to improve training stability and prove that the centralized epigraph form problem can be solved in a distributed fashion by each agent. This results in a novel centralized training distributed execution MARL algorithm which we name Def-MARL. Simulation experiments on 8 different tasks across 2 different simulators show that Def-MARL achieves the best overall performance, satisfies safety constraints, and maintains stable training. Real-world hardware experiments on Crazyflie quadcopters demonstrate the ability of Def-MARL to safely coordinate agents to complete complex collaborative tasks compared to other methods.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">RSS 2025</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ns-vimpc-480.webp 480w,/assets/img/publication_preview/ns-vimpc-800.webp 800w,/assets/img/publication_preview/ns-vimpc-1400.webp 1400w," type="image/webp" sizes="1000px"> <img src="/assets/img/publication_preview/ns-vimpc.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ns-vimpc.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="yin2025safe" class="col-sm-8"> <div class="title">Safe Beyond the Horizon: Efficient Sampling-based MPC with Neural Control Barrier Functions</div> <div class="author"> Yin Ji<sup>*</sup>, <em>Oswin So<sup>*</sup></em>, Eric Yu, Chuchu Fan, and Panagiotis Tsiotras </div> <div class="periodical"> <em>In Robotics: Science and Systems (RSS)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2502.15006" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://mit-realm.github.io/ns-vimpc" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>A common problem when using model predictive control (MPC) in practice is the satisfaction of safety beyond the prediction horizon. While theoretical works have shown that safety can be guaranteed by enforcing a suitable terminal set constraint or a sufficiently long prediction horizon, these techniques are difficult to apply and thus rarely used by practitioners, especially in the case of general nonlinear dynamics. To solve this, we make a tradeoff between exact recursive feasibility, computational tractability, and applicability to black-box dynamics by learning an approximate discrete-time control barrier function and incorporating it into variational inference MPC (VIMPC), a sampling-based MPC paradigm. To handle the resulting state constraints, we further propose a new sampling strategy that greatly reduces the variance of the estimated optimal control, improving the sample efficiency and enabling real-time planning on CPU. The resulting Neural Shield-VIMPC (NS-VIMPC) controller yields substantial safety improvements compared to existing sampling-based MPC controllers, even under badly designed cost functions. We validate our approach in both simulation and real-world hardware experiments.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR 2025</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/dgppo-480.webp 480w,/assets/img/publication_preview/dgppo-800.webp 800w,/assets/img/publication_preview/dgppo-1400.webp 1400w," type="image/webp" sizes="1000px"> <img src="/assets/img/publication_preview/dgppo.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="dgppo.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="zhang2025discrete" class="col-sm-8"> <div class="title">Discrete GCBF Proximal Policy Optimization for Multi-agent Safe Optimal Control</div> <div class="author"> Songyuan Zhang<sup>*</sup>, <em>Oswin So<sup>*</sup></em>, Mitchell Black, and Chuchu Fan </div> <div class="periodical"> <em>In The Thirteenth International Conference on Learning Representations (ICLR)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/pdf?id=1X1R7P6yzt" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/MIT-REALM/dgppo/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://mit-realm.github.io/dgppo/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Control policies that can achieve high task performance and satisfy safety contraints are desirable for any system, including multi-agent systems (MAS). One promising technique for ensuring the safety of MAS is distributed control barrier functions (CBF). However, it is difficult to design distributed CBF-based policies for MAS that can tackle unknown discrete-time dynamics, partial observability, changing neighborhoods, and input constraints, especially when a distributed high-performance nominal policy that can achieve the task is unavailable. To tackle these challenges, we propose DGPPO, a new framework that simultaneously learns both a discrete graph CBF which handles neighborhood changes and input constraints, and a distributed high-performance safe policy for MAS with unknown discrete-time dynamics. We empirically validate our claims on a suite of multi-agent tasks spanning three different simulation engines. The results suggest that, compared with existing methods, our DGPPO framework obtains policies that achieve high task performance (matching baselines that ignore the safety constraints), and high safety rates (matching the most conservative baselines), with a constant set of hyperparameters across all environments.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">T-RO</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/gcbf+-480.webp 480w,/assets/img/publication_preview/gcbf+-800.webp 800w,/assets/img/publication_preview/gcbf+-1400.webp 1400w," type="image/webp" sizes="1000px"> <img src="/assets/img/publication_preview/gcbf+.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="gcbf+.jpeg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="zhang2024gcbf+" class="col-sm-8"> <div class="title">GCBF+: A neural graph control barrier function framework for distributed safe multi-agent control</div> <div class="author"> Songyuan Zhang<sup>*</sup>, <em>Oswin So<sup>*</sup></em>, Kunal Garg, and Chuchu Fan </div> <div class="periodical"> <em>IEEE Transactions on Robotics (T-RO)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2401.14554" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/MIT-REALM/gcbfplus/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://mit-realm.github.io/gcbfplus-website" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Distributed, scalable, and safe control of large-scale multi-agent systems (MAS) is a challenging problem. In this paper, we design a distributed framework for safe multi-agent control in large-scale environments with obstacles, where a large number of agents are required to maintain safety using only local information and reach their goal locations. We introduce a new class of certificates, termed graph control barrier function (GCBF), which are based on the well-established control barrier function (CBF) theory for safety guarantees and utilize a graph structure for scalable and generalizable distributed control of MAS. We develop a novel theoretical framework to prove the safety of an arbitrary-sized MAS with a single GCBF. We propose a new training framework GCBF+ that uses graph neural networks (GNNs) to parameterize a candidate GCBF and a distributed control policy. The proposed framework is distributed and is capable of directly taking point clouds from LiDAR, instead of actual state information, for real-world robotic applications. We illustrate the efficacy of the proposed method through various hardware experiments on a swarm of drones with objectives ranging from exchanging positions to docking on a moving target without collision. Additionally, we perform extensive numerical experiments, where the number and density of agents, as well as the number of obstacles, increase. Empirical results show that in complex environments with nonlinear agents (e.g., Crazyflie drones) GCBF+ outperforms the handcrafted CBF-based method with the best performance by up to 20% for relatively small-scale MAS for up to 256 agents, and leading reinforcement learning (RL) methods by up to 40% for MAS with 1024 agents. Furthermore, the proposed method does not compromise on the performance, in terms of goal reaching, for achieving high safety rates, which is a common trade-off in RL-based methods.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS 2024</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/rcppo-480.webp 480w,/assets/img/publication_preview/rcppo-800.webp 800w,/assets/img/publication_preview/rcppo-1400.webp 1400w," type="image/webp" sizes="1000px"> <img src="/assets/img/publication_preview/rcppo.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="rcppo.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="so2024solving" class="col-sm-8"> <div class="title">Solving Minimum-Cost Reach Avoid using Reinforcement Learning</div> <div class="author"> <em>Oswin So<sup>*</sup></em>, Cheng Ge<sup>*</sup>, and Chuchu Fan </div> <div class="periodical"> <em>In Thirty-Eighth Conference on Neural Information Processing Systems (NeurIPS)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://oswinso.xyz/assets/publications/So_2024_Solving_Minimum-Cost_Reach_Avoid_using_RL.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://oswinso.xyz/rcppo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Current reinforcement-learning methods are unable to directly learn policies that solve the minimum cost reach-avoid problem to minimize cumulative costs subject to the constraints of reaching the goal and avoiding unsafe states, as the structure of this new optimization problem is incompatible with current methods. Instead, a surrogate problem is solved where all objectives are combined with a weighted sum. However, this surrogate objective results in suboptimal policies that do not directly minimize the cumulative cost. In this work, we propose RC-PPO, a reinforcement-learning-based method for solving the minimum-cost reach-avoid problem by using connections to Hamilton-Jacobi reachability. Empirical results demonstrate that RC-PPO learns policies with comparable goal-reaching rates to while achieving up to 57% lower cumulative costs compared to existing methods on a suite of minimum-cost reach-avoid benchmarks on the Mujoco simulator.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICRA 2024</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/pncbf-480.webp 480w,/assets/img/publication_preview/pncbf-800.webp 800w,/assets/img/publication_preview/pncbf-1400.webp 1400w," type="image/webp" sizes="1000px"> <img src="/assets/img/publication_preview/pncbf.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="pncbf.gif" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="so2024train" class="col-sm-8"> <div class="title">How to train your neural control barrier function: Learning safety filters for complex input-constrained systems</div> <div class="author"> <em>Oswin So</em>, Zachary Serlin, Makai Mann, Jake Gonzales, Kwesi Rutledge, Nicholas Roy, and Chuchu Fan </div> <div class="periodical"> <em>In 2024 International Conference on Robotics and Automation (ICRA)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2310.15478" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/mit-realm/pncbf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://mit-realm.github.io/pncbf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Control barrier functions (CBF) have become popular as a safety filter to guarantee the safety of nonlinear dynamical systems for arbitrary inputs. However, it is difficult to construct functions that satisfy the CBF constraints for high relative degree systems with input constraints. To address these challenges, recent work has explored learning CBFs using neural networks via neural CBF (NCBF). However, such methods face difficulties when scaling to higher dimensional systems under input constraints. In this work, we first identify challenges that NCBFs face during training. Next, to address these challenges, we propose policy neural CBF (PNCBF), a method of constructing CBFs by learning the value function of a nominal policy, and show that the value function of the maximum-over-time cost is a CBF. We demonstrate the effectiveness of our method in simulation on a variety of systems ranging from toy linear systems to an F-16 jet with a 16-dimensional state space. Finally, we validate our approach on a two-agent quadcopter system on hardware under tight input constraints.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">RSS 2023</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/efppo-480.webp 480w,/assets/img/publication_preview/efppo-800.webp 800w,/assets/img/publication_preview/efppo-1400.webp 1400w," type="image/webp" sizes="1000px"> <img src="/assets/img/publication_preview/efppo.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="efppo.gif" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="so2023solving" class="col-sm-8"> <div class="title">Solving Stabilize-Avoid Optimal Control via Epigraph Form and Deep Reinforcement Learning</div> <div class="author"> <em>Oswin So</em> and Chuchu Fan </div> <div class="periodical"> <em>In Robotics: Science and Systems (RSS)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2305.14154" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/mit-realm/efppo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://mit-realm.github.io/efppo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Tasks for autonomous robotic systems commonly require stabilization to a desired region while maintaining safety specifications. However, solving this multi-objective problem is challenging when the dynamics are nonlinear and high-dimensional, as traditional methods do not scale well and are often limited to specific problem structures. To address this issue, we propose a novel approach to solve the stabilize-avoid problem via the solution of an infinite-horizon constrained optimal control problem (OCP). We transform the constrained OCP into epigraph form and obtain a two-stage optimization problem that optimizes over the policy in the inner problem and over an auxiliary variable in the outer problem. We then propose a new method for this formulation that combines an on-policy deep reinforcement learning algorithm with neural network regression. Our method yields better stability during training, avoids instabilities caused by saddle-point finding, and is not restricted to specific requirements on the problem structure compared to more traditional methods. We validate our approach on different benchmark tasks, ranging from low-dimensional toy examples to an F16 fighter jet with a 17-dimensional state space. Simulation results show that our approach consistently yields controllers that match or exceed the safety of existing methods while providing ten-fold increases in stability performance from larger regions of attraction.</p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Oswin So. Last updated: February 10, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-FKDM3KH86Y"></script> <script defer src="/assets/js/google-analytics-setup.js"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>