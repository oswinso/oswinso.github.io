<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://oswinso.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://oswinso.github.io/" rel="alternate" type="text/html" /><updated>2025-04-23T02:34:38+00:00</updated><id>https://oswinso.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Problems with Almost-Sure Guarantees of Stochastic CBFs.</title><link href="https://oswinso.github.io/blog/2023/stochastic-cbf-almost-sure/" rel="alternate" type="text/html" title="Problems with Almost-Sure Guarantees of Stochastic CBFs." /><published>2023-09-29T00:00:00+00:00</published><updated>2023-09-29T00:00:00+00:00</updated><id>https://oswinso.github.io/blog/2023/stochastic-cbf-almost-sure</id><content type="html" xml:base="https://oswinso.github.io/blog/2023/stochastic-cbf-almost-sure/"><![CDATA[<p>I’ve always found Andrew Clark’s proofs on Stochastic CBFs <d-cite key="clark2019control,clark2021control"></d-cite> to be very impressive.
Specifically, it was very impressive that stochastic CBFs could guarantee that safety holds <em>almost-surely</em> for stochastic systems in continuous time. 
In fact, a little too impressive. Consequently, while trying to see how the proof worked, I discovered a potential mistake in the proof related to the technicalities of stochastic processes.</p>

<p>To lay the context, consider the following SDE (making the usual regularity assumptions to ensure existence).</p>

<p>\begin{equation}
    dx_t = \Big( f(x_t) + g(x_t) u_t \Big) dt + \sigma(x_t) dW_t
\end{equation}</p>

<p>Stochastic (zero)-CBFs are defined as a function $h$ that satisfies the following stochastic-analogue of the deterministic CBF condition.
Namely, for all $x$ such that $h(x) &gt; 0$, there exists a $u$ satisfying the condition</p>

<p>\begin{equation} \label{eq:scbf_condition}
\frac{\partial h}{\partial x} \Big( f(x) + g(x) u \Big) + \frac{1}{2}\mathop{tr}\left[ \sigma \sigma^\mathsf{T} \frac{\partial^2 h}{\partial x^2} \right] \geq -h(x)
\end{equation}</p>

<p>Define the zero superlevel set of $h$ as $\mathcal{C}$, i.e., $\mathcal{C} \coloneqq \{ x \mid h(x) \geq 0 \}$
Then, it is shown in <d-cite key="clark2021control"></d-cite> that the satisfaction of \eqref{eq:scbf_condition} guarantees that $\mathcal{C}$ is forward-invariant <em>almost-surely</em>:</p>

<div class="false-theorem" text="Almost-Sure Safety of Stochastic CBFs">
If $u$ satisfies \eqref{eq:scbf_condition}, then $\mathop{Pr}(x_t \in \mathcal{C}\; \forall t) = 1$ provided that $x_0 \in \mathcal{C}$.
</div>

<h2 id="problems-when-used-on-brownian-motion">Problems when used on Brownian Motion</h2>
<p>To illustrate the problem in the proof, we first consider a simpler scenario by taking $f = 0, g=0, \sigma=1$, giving us the SDE</p>

<p>\begin{equation}
    dx_t = dW_t
\end{equation}</p>

<p>In other words, $x_t = W_t$ is exactly Brownian motion. If we next define the function $h$ as</p>

<p>\begin{equation}
    h(x) \coloneqq x
\end{equation}</p>

<p>We can then verify that \eqref{eq:scbf_condition} easily holds in this case, since, for $h(x) &gt; 0$,</p>

<p>\begin{equation} \label{eq:scbf_condition_W}
     0 \geq -h(x)
\end{equation}</p>

<p>In this case, $\mathcal{C} \coloneqq \{ x \mid x \geq 0 \}$. The theorem then reduces to the following <em>alarming</em> result (using that $x_t = W_t$).</p>

<div class="false-theorem" text="Brownian Motion is Non-Negative Almost-Surely.">
    Suppose $W_t \geq 0$. Then, $\mathop{Pr}(W_t \geq 0\; \forall t) = 1$.
</div>

<p>However, since Brownian motion has unbounded support, we know that this cannot be true. To try and see how we came to this conclusion, we now trace through the proof technique from <d-cite key="clark2021control"></d-cite> applied to this simple example.</p>

<h2 id="tracing-through-the-proof">Tracing through the proof</h2>
<p>We now follow the proof from <d-cite key="clark2021control"></d-cite> for this special case of having $x=W$.</p>

<blockquote>
It is sufficient to show that, for any $t &gt; 0$, and any $\epsilon &gt; 0$, and any $\delta \in (0, 1)$,

\begin{equation}
    \mathop{Pr}\left( \inf_{t' &lt; t} W(t') &lt; -\epsilon \right) &lt; \delta.
\end{equation}

Let $\theta = \min\left\{ \frac{\delta \epsilon}{2t}, W(0) \right\}$. We now construct a sequence of <strong>stopping times</strong> $\eta_i$ and $\zeta_i$ for $i=0, 1, \dots$ as

\begin{align}
    \eta_0 &amp;= 0, \quad \zeta_0 = \inf\Big\{ t : W(t) &gt; \theta \Big\}, \newline
    \eta_i &amp;= \inf \Big\{ t : W(t) &lt; \theta, \; t &gt; \zeta_{i-1} \Big\}, \quad i = 1, 2, \dots \newline
    \zeta_i &amp;= \inf \Big\{ t : W(t) &gt; \theta, \; t &gt; \eta_{i-1} \Big\}, \quad i = 1, 2, \dots \newline
\end{align}
</blockquote>

<p>One problem here is that the random variables $\zeta_1$ and $\eta_i, \zeta_i$ for $i&gt;0$ are not elements of $\mathcal{F}_t$ but rather $\mathcal{F}_t+$, since the sets $\{ x \mid x &gt; \theta \}$ are not closed.
Consequently, these random variables are <strong>not</strong> stopping times.</p>

<p>However, this is not the only problem. For now, we assume this does not matter and continue with the proof.</p>

<blockquote>
Define the supermartingale $U_t$ as

\begin{equation}
    U_t = \theta + \sum_{i=0}^\infty \left( \int_{\eta_i \land t}^{\zeta_i \land t} -\theta \, d\tau + \int_{\eta_i \land t}^{\zeta_i \land t} dW_\tau \right) 
\end{equation}

such that

\begin{equation}
    \mathbb{E}[U_t \mid U_s] = U_s + \mathbb{E}\left[ \sum_{i=0}^\infty \int_{\eta_i \land t}^{\zeta_i \land t} -\theta \, d\tau \right] \leq U_s.
\end{equation}

We will prove by induction that $W(t) \geq U_t$ and $U_t \leq \theta$. Initially, $U_0 = \theta \leq W(0)$ by construction.

<div>
<strong>Case 1:</strong>
</div>
Suppose that the result holds up to time $t \in [\eta_i, \zeta_i]$ for all $i \geq 0$. Then, $W_t$ and $U_t$ are given by

\begin{align}
    W_t &amp;= W_{\eta_i} + \int_{\eta_i}^t dW_\tau \newline
    U_t &amp;= U_{\eta_i} + \int_{\eta_i}^t -\theta\, d\tau + \int_{\eta_i}^t dW_\tau
\end{align}

Since $W(t) \leq \theta$ for $t \in [\eta_i, \zeta_i]$ by definition of $\eta$ and $\zeta$, \eqref{eq:scbf_condition_W} implies that

\begin{equation}
    0 \geq -h(W(t)) = -W(t) \geq -\theta \quad \implies \quad \theta \geq 0.
\end{equation}

Hence, combining this with the inductive hypothesis $W_{\eta_i} \geq U_{\eta_i}$ gives us

\begin{equation}
    W_t - U_t = \left( W_{\eta_i} - U_{\eta_i} \right) + \int_{\eta_i}^t \theta\, d\tau \geq 0.
\end{equation}

Thus, $W(t) \geq U_t$. Furthermore, for $t \in [\eta_i, \zeta_i]$, $W(t) \leq \theta$, hence $U_t \leq \theta$, completing the inductive step for this case.

<div>
<strong>Case 2:</strong>
</div>
Now, suppose that the result holds up to time $t \in [\zeta_i, \eta_{i+1}]$, we have that

\begin{equation}
    U_t = U_{\zeta_i} \leq W(\zeta_i) = \theta \leq W(t)
\end{equation}

by definition of $\zeta_i$. Hence, by induction,

\begin{equation} \label{eq:induction_conclusion}
    W(t) \geq U_t, \quad \text{and} \quad U_t \leq \theta.
\end{equation}
</blockquote>

<p>One problem with this proof by induction is that it is not explicit <em>for what values of $t$</em> this statement is valid for.
The proof takes this to be all $t \geq 0$, but the structure of the induction means that this only holds over the union of the intervals $[\eta_i, \zeta_i]$ and $[\zeta_i, \eta_{i+1}]$.
For this to be equal to $t \geq 0$ requires that $\lim_{i \to \infty} \xi_i = \lim_{i \to \infty} \zeta_i = \infty$.</p>

<p>However, this is <strong>not</strong> the case, since it is possible that $\eta_i = \zeta_i = \eta_1$ for all $i \geq 1$.
Actually, this happens <em>almost surely</em>, since $W$ will go above and below $\theta$ immediately after hitting $\theta$.
Without loss of generality, suppose $W(0) = \theta$. Then,</p>

<p>\begin{equation}
    \inf\{ t &gt; 0 \mid W(t) &gt; \theta \} = \inf\{ t &gt; 0 \mid W(t) &lt; \theta \} = 0,\; \textrm{a.s.}
\end{equation}</p>

<p>The proof then continues by assuming that \eqref{eq:induction_conclusion} to be true for all $t \geq 0$, which then leads to the incorrect conclusion.</p>

<div hidden="">
<blockquote>
Since $U_t \leq W(t)$, we have that

\begin{equation}
    \mathop{Pr}\left( \inf_{t' &lt; t} W(t') &lt; -\epsilon\right) \leq \mathop{Pr}\left( \inf_{t' &lt; t} U(t') &lt; -\epsilon\right)
\end{equation}

Applying Doob's Martingale Inequality gives us that

\begin{equation}
    \mathop{Pr}\left( \inf_{0 \leq t \leq T} U_t &lt; -\epsilon \right) \leq \frac{\mathbb{E}[\max(-U_T, 0)]}{\epsilon}.
\end{equation}

We can bound $\mathbb{E}[U_T]$ by noting that

\begin{equation}
    \mathbb{E}[U_T] = \theta + \mathbb{E}\left[ \sum_{i=0}^\infty \int_{\eta_i \land t}^{\zeta_i \land t} -\theta \, d\tau \right] \geq \theta - \theta T
\end{equation}

We also know that $U_T \leq \theta$ almost surely, hence $\mathbb{E}[\max(U_T, 0)] \leq \theta$.
Combining these two statements gives us that

\begin{equation}
    \mathbb{E}[\max(-U_T, 0)] \leq \theta -\theta + \theta T = \theta T
\end{equation}

which gives us
\begin{align}
P\left( \inf_{0 \leq t \leq T} B_t &lt; -\epsilon \right) \leq \frac{\theta T}{\epsilon} = \frac{\delta \epsilon}{2T} \frac{T}{\epsilon} &lt; \delta
\end{align}

</blockquote>
</div>

<h1 id="citation">Citation</h1>
<p>Cited as:</p>
<blockquote>
  <p>So, Oswin. (Sep 2023). Problems with Almost-Sure Guarantees of Stochastic CBFs. Oswin’s Blog. https://oswinso.xyz/blog/2023/stochastic-cbf-almost-sure/</p>
</blockquote>

<p>Or</p>
<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">so2023potential</span><span class="p">,</span>
  <span class="na">title</span>   <span class="p">=</span> <span class="s">"Problems with Almost-Sure Guarantees of Stochastic CBFs"</span><span class="p">,</span>
  <span class="na">author</span>  <span class="p">=</span> <span class="s">"So, Oswin"</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">"oswinso.xyz"</span><span class="p">,</span>
  <span class="na">year</span>    <span class="p">=</span> <span class="s">"2023"</span><span class="p">,</span>
  <span class="na">month</span>   <span class="p">=</span> <span class="s">"Sep"</span><span class="p">,</span>
  <span class="na">url</span>     <span class="p">=</span> <span class="s">"https://oswinso.xyz/blog/2023/stochastic-cbf-almost-sure/"</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name>Oswin So</name></author><category term="math" /><category term="cbf" /><category term="safety" /><summary type="html"><![CDATA[I’ve always found Andrew Clark’s proofs on Stochastic CBFs to be very impressive. Specifically, it was very impressive that stochastic CBFs could guarantee that safety holds almost-surely for stochastic systems in continuous time. In fact, a little too impressive. Consequently, while trying to see how the proof worked, I discovered a potential mistake in the proof related to the technicalities of stochastic processes.]]></summary></entry><entry><title type="html">Quotient of Gaussian Densities</title><link href="https://oswinso.github.io/blog/2022/quotient-of-gaussian-densities/" rel="alternate" type="text/html" title="Quotient of Gaussian Densities" /><published>2022-02-17T00:00:00+00:00</published><updated>2022-02-17T00:00:00+00:00</updated><id>https://oswinso.github.io/blog/2022/quotient-of-gaussian-densities</id><content type="html" xml:base="https://oswinso.github.io/blog/2022/quotient-of-gaussian-densities/"><![CDATA[<h1 id="statement">Statement</h1>

<p>Let $p$ and $q$ denote Gaussian pdfs with the form</p>

<p>\begin{align}
p(x) &amp;= Z_p^{-1} \exp\left( -\frac{1}{2} (x-\mu_p)\T \Sigma_p^{-1} (x-\mu_p) \right) \newline
q(x) &amp;= Z_q^{-1} \exp\left( -\frac{1}{2} (x-\mu_q)\T \Sigma_q^{-1} (x-\mu_q) \right)
\end{align}</p>

<p>Our goal is to show that the quotient $\frac{p(x)}{q(x)}$ takes the form</p>

<p>\begin{equation}
\frac{p(x)}{q(x)} = w^{-1} \exp \left( -\frac{1}{2} (x - \mu)\T \Sigma^{-1} (x-\mu) \right)
\end{equation}</p>

<p>where
\begin{align}
\Sigma &amp;\coloneqq (\Sigma_p^{-1} - \Sigma_q^{-1})^{-1} \newline
\mu &amp;\coloneqq \Sigma (\Sigma_p^{-1} \mu_p - \Sigma_q^{-1} \mu_q) \newline
w^{-1} &amp;\coloneqq \frac{ \lvert \Sigma_p \rvert^{\frac{1}{2}} }{ \lvert \Sigma_p\rvert^{\frac{1}{2}} }
\exp\left(-\frac{1}{2} \Big[ \mu_p\T \Sigma_p^{-1} \mu_p - \mu_q\T \Sigma_q^{-1} - 
\mu\T \Sigma^{-1} \mu
\Big] \right)
\end{align}</p>

<p>Note that $\frac{p(x)}{q(x)}$ is <strong>not normalized</strong> and hence is not necessarily a probability density function.</p>

<h1 id="derivation">Derivation</h1>

<p>We first note the formula for completing the square:</p>

<p>\begin{equation}
\frac{1}{2} x\T P x - q\T x
= \frac{1}{2} (x - P^{-1} q)\T P (x - P^{-1} q) - \frac{1}{2} q\T P^{-1} q
\end{equation}</p>

<p>Now,</p>

<p>\begin{align}
&amp;\mathrel{\phantom{=}} \frac{1}{2} (x - \mu_p)\T \Sigma_p^{-1} (x - \mu_p) -
\frac{1}{2} (x - \mu_q)\T \Sigma_q^{-1} (x - \mu_q) \newline
&amp;= \frac{1}{2} x\T (\Sigma_p^{-1} - \Sigma_q^{-1}) x -
(\Sigma_p^{-1} \mu_p - \Sigma_q^{-1} \mu_q)\T x +
    \frac{1}{2} \mu_p\T \Sigma_p^{-1} \mu_p -
    \frac{1}{2} \mu_q\T \Sigma_q^{-1} \mu_q
\end{align}
\shortintertext{Defining $m \coloneqq \Sigma_p^{-1} \mu_p - \Sigma_q^{-1} \mu_q$ and $\Sigma$ as above,}
\begin{align}
&amp;= \frac{1}{2} x\T \Sigma^{-1} x - m\T x + 
    \frac{1}{2} \mu_p\T \Sigma_p^{-1} \mu_p - 
    \frac{1}{2} \mu_q\T \Sigma_q^{-1} \mu_q \newline
&amp;= \frac{1}{2} (x - \mu)\T \Sigma^{-1} (x - \mu) -
    \frac{1}{2} \mu\T \Sigma^{-1} \mu +
    \frac{1}{2} \mu_p\T \Sigma_p^{-1} \mu_p -
    \frac{1}{2} \mu_q\T \Sigma_q^{-1} \mu_q \newline
&amp;= \frac{1}{2} \Big[ (x - \mu)\T \Sigma^{-1} (x - \mu) +
    \mu_p\T \Sigma_p^{-1} \mu_p - 
    \mu_q\T \Sigma_q^{-1} -
    \mu\T \Sigma^{-1} \mu \Big]
\end{align}</p>

<p>Finally,</p>

<p>\begin{align}
\frac{ p(x) }{ q(x) }
&amp;= \frac{ \lvert \Sigma_q \rvert^{\frac{1}{2}} }{ \lvert \Sigma_p \rvert^{\frac{1}{2}} }
\exp\left( -\frac{1}{2} \Big[
(x - \mu_p)\T \Sigma_p^{-1} (x - \mu_p) -
(x - \mu_q)\T \Sigma_q^{-1} (x - \mu_q)
\Big]\right) \newline
&amp;= \frac{ \lvert \Sigma_q \rvert^{\frac{1}{2}} }{ \lvert \Sigma_p \rvert^{\frac{1}{2}} }
\exp\left(-\frac{1}{2} \Big[
(x - \mu)\T \Sigma^{-1} (x - \mu) +
    \mu_p\T \Sigma_p^{-1} \mu_p -
    \mu_q\T \Sigma_q^{-1} -
    \mu\T \Sigma^{-1} \mu
\Big]\right) \newline
&amp;= w^{-1} \exp\left( -\frac{1}{2} (x - \mu)\T \Sigma^{-1} (x - \mu) \right)
\end{align}</p>

<p>where</p>

<p>\begin{equation}
w^{-1} \coloneqq
\frac{ \lvert \Sigma_q \rvert^{\frac{1}{2}} }{ \lvert \Sigma_p \rvert^{\frac{1}{2}} }
\exp\left(-\frac{1}{2} \Big[
\mu_p\T \Sigma_p^{-1} \mu_p -
    \mu_q\T \Sigma_q^{-1} -
    \mu\T \Sigma^{-1} \mu
\Big] \right)
\end{equation}</p>]]></content><author><name></name></author><category term="math" /><summary type="html"><![CDATA[Quick derivation of quotient of Gaussian densities so it's somewhere.]]></summary></entry></feed>